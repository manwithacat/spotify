"""
Train XGBoost Model on CLEANED Spotify Dataset (78K samples) with MLflow Logging

This script trains on the enhanced cleaned dataset (v2):
1. Using deduplicated dataset (30,533 audio duplicates removed)
2. Zero-popularity tracks removed (5,157 catalog artifacts filtered)
3. 78,310 high-quality tracks with actual listener engagement
4. Optuna hyperparameter tuning (50 trials)
5. Comprehensive MLflow tracking and logging
"""

import os
import sys
import json
import warnings
from datetime import datetime
from pathlib import Path

import pandas as pd
import numpy as np
import joblib
import mlflow
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import optuna

# Suppress warnings
warnings.filterwarnings('ignore')

# Set random seed
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ============================================================================
# PATHS
# ============================================================================
BASE_DIR = Path(__file__).parent.parent
DATA_PATH = BASE_DIR / "data" / "processed" / "cleaned_spotify_data.parquet"
OUTPUTS_DIR = BASE_DIR / "outputs"
PLOTS_DIR = OUTPUTS_DIR / "plots"
MODELS_DIR = OUTPUTS_DIR / "models"
METADATA_DIR = OUTPUTS_DIR / "metadata"

# Create directories
for dir_path in [OUTPUTS_DIR, PLOTS_DIR, MODELS_DIR, METADATA_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

print("="*80)
print("üéµ TRAINING ON CLEANED SPOTIFY DATASET V2 (78K SAMPLES) + MLFLOW")
print("="*80)
print("üìä Enhanced dataset: Deduplicated + Zero-popularity removed")
print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# ============================================================================
# STEP 1: LOAD FULL DATASET
# ============================================================================
print("\n" + "="*80)
print("üìÇ STEP 1: LOAD FULL DATASET")
print("="*80)

df = pd.read_parquet(DATA_PATH)
print(f"‚úÖ Loaded data: {len(df):,} rows, {df.shape[1]} columns")

# ============================================================================
# STEP 2: FEATURE SELECTION
# ============================================================================
print("\n" + "="*80)
print("üîß STEP 2: FEATURE SELECTION")
print("="*80)

# Use only core audio features that exist and are meaningful
# DO NOT use release_year - it doesn't exist in the dataset!
feature_cols = [
    'danceability',
    'energy',
    'loudness',
    'speechiness',
    'acousticness',
    'instrumentalness',
    'liveness',
    'valence',
    'tempo'
]

# Verify all features exist
missing_features = [f for f in feature_cols if f not in df.columns]
if missing_features:
    print(f"‚ö†Ô∏è  Missing features: {missing_features}")
    feature_cols = [f for f in feature_cols if f in df.columns]

target_col = 'popularity'

print(f"\n‚úì Target: {target_col}")
print(f"‚úì Features ({len(feature_cols)}): {feature_cols}")

# Extract features and target
X = df[feature_cols].copy()
y = df[target_col].copy()

# Remove any rows with NaN
mask = ~(X.isnull().any(axis=1) | y.isnull())
X = X[mask]
y = y[mask]

print(f"\n‚úÖ Clean dataset: {len(X):,} samples, {len(feature_cols)} features")

# Show feature distributions (should be natural ranges, NOT scaled)
print(f"\n‚úì Feature ranges (should be natural, not scaled):")
for col in feature_cols:
    print(f"  {col}: [{X[col].min():.3f}, {X[col].max():.3f}] "
          f"(mean: {X[col].mean():.3f}, std: {X[col].std():.3f})")

# ============================================================================
# STEP 3: TRAIN/TEST SPLIT
# ============================================================================
print("\n" + "="*80)
print("‚úÇÔ∏è  STEP 3: TRAIN/TEST SPLIT")
print("="*80)

# Split: 70% train, 15% validation, 15% test
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.15, random_state=RANDOM_STATE, shuffle=True
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.15/(1-0.15), random_state=RANDOM_STATE, shuffle=True
)

print(f"‚úì Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)")
print(f"‚úì Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)")
print(f"‚úì Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)")

# Verify distributions are similar
print(f"\n‚úì Target distribution check:")
print(f"  Train mean: {y_train.mean():.2f}, std: {y_train.std():.2f}")
print(f"  Val mean: {y_val.mean():.2f}, std: {y_val.std():.2f}")
print(f"  Test mean: {y_test.mean():.2f}, std: {y_test.std():.2f}")

# Save test/train data for later use
X_train.to_parquet(BASE_DIR / "data" / "processed" / "X_train_full.parquet")
X_test.to_parquet(BASE_DIR / "data" / "processed" / "X_test_full.parquet")
y_train.to_frame().to_parquet(BASE_DIR / "data" / "processed" / "y_train_full.parquet")
y_test.to_frame().to_parquet(BASE_DIR / "data" / "processed" / "y_test_full.parquet")
print(f"\n‚úÖ Saved train/test splits to data/processed/")

# ============================================================================
# STEP 4: OPTUNA HYPERPARAMETER TUNING
# ============================================================================
print("\n" + "="*80)
print("üéØ STEP 4: OPTUNA HYPERPARAMETER TUNING")
print("="*80)

def objective(trial):
    """Optuna objective function"""
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'random_state': RANDOM_STATE,
        'n_jobs': -1,
        'early_stopping_rounds': 50,

        # Tunable hyperparameters
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
    }

    model = XGBRegressor(**params)
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False
    )

    y_pred_val = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))

    return rmse

print("Running Optuna optimization (50 trials)...")
study = optuna.create_study(direction='minimize', study_name='spotify_xgboost')
study.optimize(objective, n_trials=50, show_progress_bar=True)

print(f"\n‚úÖ Best trial:")
print(f"  RMSE: {study.best_value:.4f}")
print(f"\n‚úÖ Best hyperparameters:")
best_params = study.best_params
for key, value in best_params.items():
    print(f"  {key}: {value}")

# ============================================================================
# STEP 5: TRAIN FINAL MODEL
# ============================================================================
print("\n" + "="*80)
print("ü§ñ STEP 5: TRAIN FINAL MODEL")
print("="*80)

# Add fixed params
final_params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'random_state': RANDOM_STATE,
    'n_jobs': -1,
    'early_stopping_rounds': 50,
    **best_params
}

# Train on full training set
model = XGBRegressor(**final_params)
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val), (X_test, y_test)],
    verbose=10
)

print(f"\n‚úÖ Model trained with {model.best_iteration} iterations")

# ============================================================================
# STEP 6: EVALUATE
# ============================================================================
print("\n" + "="*80)
print("üìä STEP 6: COMPREHENSIVE EVALUATION")
print("="*80)

# Predictions
y_pred_train = model.predict(X_train)
y_pred_val = model.predict(X_val)
y_pred_test = model.predict(X_test)

# Calculate metrics
def calculate_metrics(y_true, y_pred, set_name):
    """Calculate comprehensive metrics"""
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)

    # Adjusted R¬≤
    n = len(y_true)
    p = X_train.shape[1]
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    print(f"\n{set_name} Set:")
    print(f"  R¬≤ = {r2:.4f}")
    print(f"  Adjusted R¬≤ = {adj_r2:.4f}")
    print(f"  RMSE = {rmse:.2f}")
    print(f"  MAE = {mae:.2f}")
    print(f"  Prediction range: [{y_pred.min():.2f}, {y_pred.max():.2f}]")
    print(f"  Prediction std: {y_pred.std():.2f}")

    return {
        'r2': r2,
        'adjusted_r2': adj_r2,
        'rmse': rmse,
        'mae': mae
    }

metrics = {}
metrics['train'] = calculate_metrics(y_train, y_pred_train, "Training")
metrics['val'] = calculate_metrics(y_val, y_pred_val, "Validation")
metrics['test'] = calculate_metrics(y_test, y_pred_test, "Test")

# Check for collapse
print(f"\n" + "="*80)
print("üîç MODEL HEALTH CHECK")
print("="*80)

collapse_indicators = []

if metrics['test']['r2'] < 0:
    collapse_indicators.append("‚ùå Negative R¬≤ (model collapsed)")
elif metrics['test']['r2'] < 0.1:
    collapse_indicators.append("‚ö†Ô∏è  Very low R¬≤ (<0.1)")

if y_pred_test.std() < 5:
    collapse_indicators.append(f"‚ùå Low prediction variance ({y_pred_test.std():.2f})")

pred_range = y_pred_test.max() - y_pred_test.min()
if pred_range < 20:
    collapse_indicators.append(f"‚ùå Narrow prediction range ({pred_range:.2f})")

if not collapse_indicators:
    print("‚úÖ Model is healthy! No collapse detected.")
else:
    print("‚ö†Ô∏è  Potential issues:")
    for indicator in collapse_indicators:
        print(f"  {indicator}")

# ============================================================================
# STEP 7: SAVE MODEL
# ============================================================================
print("\n" + "="*80)
print("üíæ STEP 7: SAVE MODEL & METADATA")
print("="*80)

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
model_path = MODELS_DIR / f"xgb_model_full_{timestamp}.joblib"
metadata_path = METADATA_DIR / f"xgb_metadata_full_{timestamp}.json"

# Save model
joblib.dump(model, model_path)
print(f"‚úÖ Model saved: {model_path}")

# Save metadata
metadata = {
    'timestamp': datetime.now().isoformat(),
    'dataset': 'full_spotify_114k',
    'n_samples': len(X),
    'n_features': len(feature_cols),
    'feature_names': feature_cols,
    'model_params': final_params,
    'metrics': {
        'train_r2': metrics['train']['r2'],
        'train_rmse': metrics['train']['rmse'],
        'train_mae': metrics['train']['mae'],
        'val_r2': metrics['val']['r2'],
        'val_rmse': metrics['val']['rmse'],
        'val_mae': metrics['val']['mae'],
        'test_r2': metrics['test']['r2'],
        'test_adjusted_r2': metrics['test']['adjusted_r2'],
        'test_rmse': metrics['test']['rmse'],
        'test_mae': metrics['test']['mae'],
    },
    'data_shapes': {
        'train': list(X_train.shape),
        'val': list(X_val.shape),
        'test': list(X_test.shape)
    }
}

with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"‚úÖ Metadata saved: {metadata_path}")

# Save feature importance
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

feature_importance_path = MODELS_DIR / f"feature_importance_full_{timestamp}.csv"
feature_importance.to_csv(feature_importance_path, index=False)
print(f"‚úÖ Feature importance saved: {feature_importance_path}")

# ============================================================================
# STEP 8: LOG TO MLFLOW
# ============================================================================
print("\n" + "="*80)
print("üìä STEP 8: LOG TO MLFLOW")
print("="*80)

# Start MLflow run
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("spotify-popularity-prediction")

with mlflow.start_run(run_name=f"xgboost_cleaned_v2_{timestamp}"):

    # Log dataset info
    mlflow.log_param("dataset", "cleaned_spotify_data_v2")
    mlflow.log_param("dataset_version", "v2_deduplicated_zero_removed")
    mlflow.log_param("n_samples_total", len(X))
    mlflow.log_param("n_samples_train", len(X_train))
    mlflow.log_param("n_samples_val", len(X_val))
    mlflow.log_param("n_samples_test", len(X_test))
    mlflow.log_param("n_features", len(feature_cols))
    mlflow.log_param("features", ",".join(feature_cols))

    # Log hyperparameters
    for key, value in final_params.items():
        mlflow.log_param(key, value)

    # Log Optuna info
    mlflow.log_param("optuna_trials", 50)
    mlflow.log_metric("optuna_best_rmse", study.best_value)

    # Log training metrics
    mlflow.log_metric("train_r2", metrics['train']['r2'])
    mlflow.log_metric("train_adj_r2", metrics['train']['adjusted_r2'])
    mlflow.log_metric("train_rmse", metrics['train']['rmse'])
    mlflow.log_metric("train_mae", metrics['train']['mae'])

    # Log validation metrics
    mlflow.log_metric("val_r2", metrics['val']['r2'])
    mlflow.log_metric("val_adj_r2", metrics['val']['adjusted_r2'])
    mlflow.log_metric("val_rmse", metrics['val']['rmse'])
    mlflow.log_metric("val_mae", metrics['val']['mae'])

    # Log test metrics (primary metrics)
    mlflow.log_metric("test_r2", metrics['test']['r2'])
    mlflow.log_metric("test_adj_r2", metrics['test']['adjusted_r2'])
    mlflow.log_metric("test_rmse", metrics['test']['rmse'])
    mlflow.log_metric("test_mae", metrics['test']['mae'])

    # Log prediction statistics
    mlflow.log_metric("pred_test_min", float(y_pred_test.min()))
    mlflow.log_metric("pred_test_max", float(y_pred_test.max()))
    mlflow.log_metric("pred_test_std", float(y_pred_test.std()))
    mlflow.log_metric("pred_test_range", float(y_pred_test.max() - y_pred_test.min()))

    # Log model health indicators
    mlflow.log_metric("model_healthy", 1 if not collapse_indicators else 0)
    mlflow.log_metric("num_health_issues", len(collapse_indicators))

    # Log artifacts
    mlflow.log_artifact(str(model_path), "models")
    mlflow.log_artifact(str(metadata_path), "metadata")
    mlflow.log_artifact(str(feature_importance_path), "feature_importance")

    # Log feature importance as metrics for easy filtering
    for idx, row in feature_importance.iterrows():
        mlflow.log_metric(f"importance_{row['feature']}", row['importance'])

    # Log tags
    mlflow.set_tag("model_type", "XGBoost")
    mlflow.set_tag("data_cleaning", "v2_deduplicated_zero_removed")
    mlflow.set_tag("optuna_optimization", "enabled")
    mlflow.set_tag("training_timestamp", timestamp)
    mlflow.set_tag("dataset_reduction", "31.31%")
    mlflow.set_tag("duplicates_removed", "30,533")
    mlflow.set_tag("zero_pop_removed", "5,157")

    # Get run ID
    run_id = mlflow.active_run().info.run_id
    print(f"\n‚úÖ MLflow run logged successfully!")
    print(f"   Run ID: {run_id}")
    print(f"   Experiment: spotify-popularity-prediction")
    print(f"   View at: http://localhost:5000")

print(f"\n" + "="*80)
print("‚úÖ TRAINING COMPLETE")
print("="*80)
print(f"\nFinal Test Performance:")
print(f"  R¬≤ = {metrics['test']['r2']:.4f}")
print(f"  RMSE = {metrics['test']['rmse']:.2f}")
print(f"  MAE = {metrics['test']['mae']:.2f}")
print(f"\nTop 5 Features:")
for idx, row in feature_importance.head(5).iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

print(f"\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)
